{"meta":{"title":"领江的小博客","subtitle":"记录在学校挣扎的苦逼生活...","description":null,"author":"Dany Xie","url":"http://sowhatxie.top"},"pages":[{"title":"我呀。。","date":"2017-07-12T12:29:13.000Z","updated":"2017-07-12T12:31:28.177Z","comments":true,"path":"about/index.html","permalink":"http://sowhatxie.top/about/index.html","excerpt":"","text":"挣扎中，欢迎小姐姐来拯救我哦！"}],"posts":[{"title":"复盘一阶段","slug":"复盘一阶段","date":"2017-08-14T07:17:10.000Z","updated":"2017-07-15T09:01:56.003Z","comments":true,"path":"2017/08/14/复盘一阶段/","link":"","permalink":"http://sowhatxie.top/2017/08/14/复盘一阶段/","excerpt":"学习方法领域内，国际顶级会议：internation conference on data mining internation conference on data engineering internation conference on machine learning internation joint conference on artificial intelligence pacific-asia conference on knowledge discoverry and data mining acm sigkdd conference on knowledge iscoverry and data mining 还有一些IEEE的领域内顶级期刊 跟踪领域内，专家 必须用Google 搜文献 利用 machine learning repository UCI 来收集数据集","text":"学习方法领域内，国际顶级会议：internation conference on data mining internation conference on data engineering internation conference on machine learning internation joint conference on artificial intelligence pacific-asia conference on knowledge discoverry and data mining acm sigkdd conference on knowledge iscoverry and data mining 还有一些IEEE的领域内顶级期刊 跟踪领域内，专家 必须用Google 搜文献 利用 machine learning repository UCI 来收集数据集还有 kd nuggets 这个网站 基于此，掌握领域前沿开阔视野 数据挖掘，是一个综合的领域，多学科交叉 /Users/DANNY/Documents/博客内图片/7.15涉及的小方面.png 学习方法： 课堂或看视频 思考与讨论 阅读相关材料 扩展与启发 联系 技术技巧与应用 开放数据如何寻找，下载数据？ 分类监督学习 专业名词： 分类边界 classification boundaries 过拟合 overfitting 交叉验证 cross validation （训练集 测试集） confusion matrix 混淆矩阵 ： 假设一个二分类问题 样本本身具有 positive negative ，此外训练得到模型也会给样本进行预测，给出预测值 positive negative 因此基于此，衍生出四种情况 ： true positive TP （本来的标签就是 positive ，模型给出的预测值也是 positive） false negative FN (本来的标签是 positive ，模型给出的预测值是 negative) false positive FP (本来的标签是 negative ，模型给出的预测值是 positive) true negative TN （本来的标签就是 negative ，模型给出的预测值也是 negative） 基于这四种情况，给出这些统计量：略 receiver operating characteristic (ROC 曲线) ，一般横坐标 FP 纵坐标 TP，FP + TP =1 。 AUC 曲线下方的面积 cost sensitive learning 不同领域侧重点不同（权重不同），错误代价不同。lift analysis 利用模型相较于不用模型，提升度有多少。 聚类及其他挖掘问题非监督学习 层次聚类，非层次聚类： 距离度量（相似度）： 关联规则（association rule）： 回归：过拟合等 算法： 应用： 可视化（seeing is knowing ）： 可解释性 performance dashboard 数据预处理（data preprocessing ）：数据质量 是很基础却又很关键的一步 数据挖掘是挖掘规律，而不产生规律 注意内在规律 相关性不等于因果 幸存者偏执（survivorship bias ） 不清楚这点，可能会本末倒置 盲人摸象 看问题要全面 隐私保护与并行计算数据预处理 （data preprocessing）总纲： data cleaning data transformation data description feature selection feature extraction 数据来源 来源，格式均不同数据预处理，可能是整个数据挖掘过程中最具有挑战性的工作。 脏数据：不完全，缺失（采集设备故障等原因；采集时，受试者未提供；数据不适合 N/A）； 种类：完全随机缺失，条件随机缺失，等等。有噪声；冗余。。。。 处理手段：忽视；填缺失值（自动；凭借领域知识，经验）； 离群与重复值检测处理：离群点（Outliers）离群是一个相对的概念，如何度量？异常点（anomaly） duplicate data （重复检测）：从冗余信息中，区分出是否有些东西指的是同一个东西 技术：滑动窗口；哈希散列（相近的的给出相近的哈希值）； 类型转换与采样： 对不同类型，采用合适的编码方式？低维的转到高维来处理。。。。。 利用采样，降低处理的复杂度。aggregation ,调整比例。。。。 对于本身就不平衡的数据集，如何处理？不能完全依据准确率来评判分类器的性能（评判癌症）。G-mean，F-measure，over-sampling,boundary-sampling(边缘采样) 数据描述与可视化normalization(标准化)：min-max（明确上下界）；z-score（根据均值）；数据描述的一些统计量：均值，中位数，mode，variance。。。可视化：不同的维度；各种不同的可视化方式（各种XXX图） 特征选择（feature selection）去除一些不要的属性；好的属性，能够有很好的区分度。如何量化？利用熵（entropy）量化区分度。信息增益。 branch and bound ：单调性假设；剪枝；top k individual features；sequential forward selection ;sequential backword selection ; 主成分分析（PCA）特征的压缩，而非去除。角度不同，得到的信息不同。坐标轴旋转，转换矩阵。降维，投影；拉格朗日乘数法，把约束的优化，转化为无约束优化。","categories":[{"name":"笔记","slug":"笔记","permalink":"http://sowhatxie.top/categories/笔记/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sowhatxie.top/tags/ML/"}]},{"title":"范数","slug":"范数","date":"2017-07-14T07:17:10.000Z","updated":"2017-07-25T08:50:48.608Z","comments":true,"path":"2017/07/14/范数/","link":"","permalink":"http://sowhatxie.top/2017/07/14/范数/","excerpt":"","text":"范数 训练模型的目的就是让训练出的模型在测试集上有很小的误差。（而不是追求最小的训练误差，虽然最终的目的是想要在将来预测时最准，但是未来是未知的，只能寻求最小的测试误差） 监督机器学习问题无非就是“minimize your error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。 最小化误差，为了让模型充分拟合训练集；规则化参数，防止模型过拟合训练集。（参数过多，模型复杂度上升，易过拟合） 所以实际操作中，通过规则化参数来使得模型足够简单（防止过拟合），在此基础上最小化误差（使参数有较好的泛化性能）。 规则化参数实现方式：通过规则函数。此外规则项可以用来约束模型的特性（可以把一些先验的领域知识经验，融合进模型的学习之中）。通常想要模型拥有的特性包括：稀疏，低秩，平滑等等。 规则化作用的其他角度： 规则化符合奥卡姆剃刀(Occam’s razor)原理。在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。 从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。 规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。 对于监督学习：最小化的目标函数一般都是由 损失函数 规则项 两部分组成。损失函数，衡量预测值与实际值的误差大小，具有不同的拟合特性。规则项就是用来对参数规则化，使得模型尽量简单。 L0范数是指向量中非0的元素的个数。目的就是使得参数大部分元素都是0. L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。 原理解释：L1范数可以使权值稀疏 -&gt; L1范数是L0范数的最优凸近似 -&gt; 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。 为什么用L1不用L0？一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。 参数稀疏的优点： 特征选择(Feature Selection)：能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)：当特征对应的权值大部分通过规则化处理后为0后，仅有少部分的权值对应为非零，这样在实际问题中，只需要对少有的几个特征进行判别就可以了，增强了解释性和操作性。 L2范数（岭回归 Ridge Regression 或 权值衰减 weight decay）：L2范数是指向量各元素的平方和然后求平方根。使L2范数对应规则项最小，就变向使得w权值的每个元素都很小，都接近于0。区别于L1范数，L2范数使得权值接近于0，但不等于0。（可以这么认为，越小的权值参数意味着模型越简单，越不容易过拟合） 网友的理解： L2范数最开始的意思应该不是让参数小，而是把参数w限制在某个范围内：min（loss function L） , 约束条件 ||w||2 &lt; M;用数学转成等价形式为min( L+入||w||2)。从新的形势看就成了参数小则model简单。 从回归理解，加入L2范数使得可逆有解；从线性分类器理解，w越小，y=wx+b，x变化很大，y才变化一点点，更stable，更general。大的权值对对函数的扰动就大。 可以解释抗干扰能力强这个特点。但我的另一个观点是：其实对于模型简单这个特点来说，L2不应该和L1比较，或者说这两种选择下得到的模型都简单，L1倾向于选择使得特征数目较少，对于多项式函数来说，也就是多项式次数比较低，这当然是一个简单的模型。而对于L2来说，多项式次数虽然高于L1，但是它的每一项系数非常小，所以在每一个样本点附近的导数（该导数代表了模型的复杂度，也就是方差）同样可以小，因此对于过拟合问题也有很好的预防能力。所以，我的结论是：在过拟合这点上，L1和L2无法进行比较，各自在其他方面有不同的侧重点而已。 解释L2范数的优点： 学习理论的角度：从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。 优化计算的角度：从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。可以让我们的优化求解变得稳定和快速。 解释 condition number ：优化的两大难题：局部最小值（我们要找的是全局最小值，如果局部最小值太多，那我们的优化算法就很容易陷入局部最小而不能自拔）；ill-condition病态问题：condition number就是用来衡量病态程度的。ill-condition对应的是well-condition。condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number值小的就是well-conditioned的，大的就是ill-conditioned的。如果方阵A是非奇异的（可逆的，满秩的），那么A的conditionnumber定义为矩阵A的norm（范数）乘以它的逆的norm。范数可以理解为用来衡量矩阵大小的。表示矩阵大小或者向量长度。总结：conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。 奇异矩阵的概念：直接的理解，矩阵奇异就是说明矩阵不是满秩的。由于理解矩阵有很多角度，可以进一步理解。从行列式角度看，若矩阵的行列式等于0，矩阵就是奇异矩阵，从矩阵是否可逆的角度，矩阵不可逆（行列式为0），说明矩阵就是奇异矩阵。从解方程组角度看，如果A为奇异矩阵，则AX=0有无穷解，AX=b有无穷解或者无解。 上面说的矩阵的秩，可以理解为组成矩阵的向量之间相关性，满秩就说明向量之间全部线性无关。 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解。然而，如果当我们的样本X的数目比每个样本的维度还要小的时候，矩阵XTX将会不是满秩的，也就是XTX会变得不可逆，所以w*就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。而如果加上L2规则项就可以直接求逆了。规则项的引入则可以改善condition number。 另外，如果使用迭代优化的算法，condition number 太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成λ-strongly convex（λ强凸）的了。 对于λ-strongly convex（λ强凸）：普通的优化问题，强调是convex函数。convex性质说明函数曲线总是位于任意点处切线的上方。而strongly convex 则进一步要求位于该处的一个二次函数上方，也就是说要求函数不要太“平坦”而是可以保证有一定的“向上弯曲”的趋势。专业点说，就是convex 可以保证函数在任意一点都处于它的一阶泰勒函数之上，而strongly convex可以保证函数在任意一点都存在一个非常漂亮的二次下界quadratic lower bound。这是一个很强的假设。仅仅靠convex 性质并不能保证在梯度下降和有限的迭代次数的情况下得到的点w会是一个比较好的全局最小点w的近似点，如果f(w)在全局最小点w周围是非常平坦的情况的话，我们有可能会找到一个很远的点。但如果我们有“强凸”的话，就能对情况做一些控制，我们就可以得到一个更好的近似解。至于有多好嘛，这里面有一个bound，这个 bound 的好坏也要取决于strongly convex性质中的常数α的大小。看到这里，不知道大家学聪明了没有。如果要获得strongly convex怎么做？最简单的就是往里面加入一项(α/2)*||w||2。总的说，在梯度下降中，目标函数收敛速率的上界实际上是和矩阵XTX的 condition number有关，XTX的 condition number 越小，上界就越小，也就是收敛速度会越快。 参考：http://blog.csdn.net/zouxy09/article/details/24971995","categories":[{"name":"笔记","slug":"笔记","permalink":"http://sowhatxie.top/categories/笔记/"}],"tags":[{"name":"ML","slug":"ML","permalink":"http://sowhatxie.top/tags/ML/"},{"name":"基础知识","slug":"基础知识","permalink":"http://sowhatxie.top/tags/基础知识/"}]},{"title":"动态规划 -> 贪心 -> 递归的优化 -> 尾递归 -> 迭代","slug":"动态规划 -> 递归的优化 -> 尾递归 -> 迭代  ","date":"2017-06-13T07:17:10.000Z","updated":"2017-07-25T08:49:12.321Z","comments":true,"path":"2017/06/13/动态规划 -> 递归的优化 -> 尾递归 -> 迭代  /","link":"","permalink":"http://sowhatxie.top/2017/06/13/动态规划 -> 递归的优化 -> 尾递归 -> 迭代  /","excerpt":"","text":"动态规划 -&gt; 贪心 -&gt; 递归的优化 -&gt; 尾递归 -&gt; 迭代动态规划&emsp;&emsp;动态规划（Dynamic Programming）和分治算法比较类似，但不同的是分治算法把原问题划归为几个相互独立的子问题，从而一一解决，而动态规划则是针对子问题有重叠的情况的一种解决方案。对于有重叠的情况，就是用一张表来保存子问题的中间结果，这样遇到重复的问题就不需要重新计算而是查表即可。 &emsp;&emsp;目前设计动态规划主要有两个思路： &emsp;&emsp;一个是利用递归方法，即首先把问题用递归的方法解决，然后用一张表保存递归中的中间结果。遇到需要计算以前计算过的东西，直接查表。 &emsp;&emsp;另一个思路是exhaust search。 &emsp;&emsp;动态规划只能应用于最优子结构问题，最优子结构的意思是局部最优解能决定全局最优解（对有些问题这个要求并不能完全满足，故有时需要引入一定的近似）。简单地说，问题能够分解成子问题来解决。 适用情况： 最优子结构性质。如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质。最优子结构性质为动态规划算法解决问题提供了重要线索。 无后效性。即子问题的解一旦确定，就不再改变，不受在这之后，包含他的最大的问题的求解决策影响。 子问题重叠性质。子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不是总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地察看一下结果，从而获得较高的效率。 动态规划的本质，是对问题状态的定义和状态转移方程的定义。 基础概念：问题的阶段与问题的状态。对于递推，无需考虑是不是需要更多的状态，也不需要选择哪些旧状态来计算新状态（递推公式本身已经决定了如 a[i]=a[i-1]+a[i-2]）。一般的情况，假如问题有n个阶段，每个阶段有多个状态，不同阶段的状态数不必相同，一个阶段的一个状态可以得到下个阶段的所有状态中的几个。那我们要计算出最终阶段的状态数自然要经历之前每个阶段的某些状态。 贪心：“下一步最优是从当前最优得到的”。为了计算最终的最优值，只需要存储每一步的最优值即可，解决这种性质的问题的算法叫做贪心。 总结： 每个阶段只有一个状态————-递推（递归是递推式求解的方法） 每个阶段的最优状态都是由上一个阶段的最优状态得到的———-贪心（每一步的最优解一定依赖上一步的最优解。） 每个阶段的最优状态是由之前所有阶段的状态的组合得到的———搜索 每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到而不管之前这个状态时如何得到的——动态规划 （全局最优解中一定包含某个局部最优解，但不一定包含前一个局部最优解，因此需要记录之前的所有最优解。） 每个阶段的最优状态可以从之前某个或某些状态直接得到，这个性质叫做最优子结构； 而不管这个状态是如何得到的，这个性质叫做无后效性。 另外，其实动态规划中的最优状态的说法容易产生误导，以为只需要计算最优状态就好，LIS问题却是如此，转移时只用了每个阶段的选的状态，但实际上有的问题往往需要对每个阶段的所有状态都作出一个最优值，然后根据这些最优值再来找最优状态。 贪心贪心算法总是作出在当前看来最好的选择。也就是说贪心算法并不从整体最优考虑，它所作出的选择只是在某种意义上的局部最优选择。虽然贪心并不保证得到全局最优，但是在某些问题，或是在某些约束下，可以得到全局的最优，或者说在一些情况下，即使贪心算法不能得到整体最优解，其最终结果却是最优解的很好近似。 对比http://www.cnblogs.com/codeskiller/p/6477181.html 递归优化之Memoization&emsp;&emsp;一般的递归有个很明显的缺点，就是可能会重复计算。有一种可以用来克服这个缺点的方法就是利用Memoization这种思路。使用表来记录中间的结果。 &emsp;&emsp;Memoization最初是用来优化计算机程序使之计算的更快的技术，是通过存储调用函数的结果并且在同样参数传进来的时候返回结果。大部分应该是在递归函数中使用。 &emsp;&emsp;Memorization 可以把函数每次的返回值存在一个数组或者对象中，在接下来的计算中可以直接读取已经计算过并且返回的数据，不用重复多次相同的计算。这种方法可用于部分递归中以提高递归的效率。 &emsp;&emsp;但是这个优化的方法有一个缺点，就是会使得调用栈会很深。如何避免这种调用栈深的情况，下面介绍尾递归。 递归优化之尾递归&emsp;&emsp;这里我只说一下最关键的一点：尾递归就是递归函数最后一步调用自己的那一步，只是单纯的调用自己，不能出现其他的什么鬼。（注意是程序执行的最后一步而非最后一行代码） &emsp;&emsp;这边有个大神讲尾递归很好直接贴出网址 ：http://www.ruanyifeng.com/blog/2015/04/tail-call.html他的博文涉及很多背景知识，也进行了相关介绍：函数的尾调用，函数调用的机制，尾递归，柯里化（currying）等。 &emsp;&emsp;另外这边的讨论也挺好，可以参考： https://www.zhihu.com/question/20761771/answer/19996299 迭代&emsp;&emsp;迭代是代码中经常使用的东西，这里讲迭代，主要是因为把递归函数转化为迭代。为什么要转化为迭代？原因在于，Java,Python等语言（C好像有优化），对于尾递归并没有优化，某些场景是需要转化为迭代处理。 结尾&emsp;&emsp;其实还有很多优化的方式，可以继续探索！","categories":[{"name":"笔记","slug":"笔记","permalink":"http://sowhatxie.top/categories/笔记/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://sowhatxie.top/tags/算法/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://sowhatxie.top/tags/LeetCode/"}]},{"title":"建站笔记","slug":"建站笔记","date":"2016-07-12T10:58:51.000Z","updated":"2017-07-14T03:36:47.097Z","comments":true,"path":"2016/07/12/建站笔记/","link":"","permalink":"http://sowhatxie.top/2016/07/12/建站笔记/","excerpt":"安装node.js准备命令： yum -y install gcc make gcc-c++ openssl-devel wget去node.js中文网下载linux 64位的安装文件 xxx.tar.xz解压缩文件 xz -d node-v8.1.3-linux-x64.tar.xz 再解压缩 tar -xvf node-v8.1.3-linux-x64.tar把文件直接移到 指定文件夹中 sudo mv node-v8.1.3-linux-x64 /usr/local配置环境变量 sudo vim /etc/profile 添加如下：export PATH=$PATH:/usr/local/nodejs/bin 生效 source /etc/profile查看是否配置成功：echo $PATH检查node.js是否配置成功 node -v npm -v","text":"安装node.js准备命令： yum -y install gcc make gcc-c++ openssl-devel wget去node.js中文网下载linux 64位的安装文件 xxx.tar.xz解压缩文件 xz -d node-v8.1.3-linux-x64.tar.xz 再解压缩 tar -xvf node-v8.1.3-linux-x64.tar把文件直接移到 指定文件夹中 sudo mv node-v8.1.3-linux-x64 /usr/local配置环境变量 sudo vim /etc/profile 添加如下：export PATH=$PATH:/usr/local/nodejs/bin 生效 source /etc/profile查看是否配置成功：echo $PATH检查node.js是否配置成功 node -v npm -v安装git:直接安装：sudo yum install git查看是否安装成功：git –version查看安装位置：which git配置git :git config –global user.name “LingJiangXie” git config –global user.email “xielj1994@outlook.com”查看配置列表：git config –list 在自己的git上新建一个 LingJiangXie.github.io（不能为空，否则没有节点）ssh 密药配置 参考http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374385852170d9c7adf13c30429b9660d0eb689dd43a000 其他基本参考：http://www.cnblogs.com/wanghuaijun/p/7073296.html 部署服务时 出现问题参考：npm install hexo-deployer-git –save （此命令在新建的本地博客文件夹根目录执行）改了之后执行，然后再部署试试 修改，更换完主题 重新执行//生成静态文件hexo generate //部署服务hexo deploy 还可以参考这个： https://zhuanlan.zhihu.com/p/26625249 比较全","categories":[{"name":"笔记","slug":"笔记","permalink":"http://sowhatxie.top/categories/笔记/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://sowhatxie.top/tags/Hexo/"},{"name":"YAYA","slug":"YAYA","permalink":"http://sowhatxie.top/tags/YAYA/"}]}]}